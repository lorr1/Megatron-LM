inherit:
    # parameters from later configs in this list will override parameters from those earlier
    # use either relative or absolute paths
    - base_gpt2_345m.yaml
    - base_file_paths.yaml
    - base_output.yaml
    - base_wandb_pretrain.yaml

# Data path
save: /u/scr/nlp/ooa/megatron-preprocessed-data/testing_outputs/gpt2_wiki103_2
load: /u/scr/nlp/ooa/megatron-preprocessed-data/testing_outputs/gpt2_wiki103_2
tensorboard_dir: /u/scr/nlp/ooa/megatron-preprocessed-data/testing_outputs/gpt2_wiki103_2/tensorboard
data_path: /u/scr/nlp/ooa/megatron-preprocessed-data/wikitext103-full/wikitext103_text_document

# Model
num_layers: 4
hidden_size: 512
num_attention_heads: 16
seq_length: 512
max_position_embeddings: 512
batch_size: 56

# Training
log_interval: 100
save_interval: 3000
eval_interval: 1000
eval_iters: 30
train_iters: 100000


# Data split
split: "0.9954271262325057,0.0024701379039286384,0.0021027358635656796"